---
title: "ED project"
author: "Jing Yeh, Saurav Sathnarayan"
date: "2024-09-03"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Contents

#### Introduction

#### Objectives

#### Sources of Variation

#### Randomisation

#### Analysis of pilot experiment

#### Our Model and Analysis

#### Summary and Conclusions

#### References

#### Apenddix

\

_things need to change and edit_
\
1. add page numbers
\
2. might need to add new pages/headings depending on project length.
\
3. Add title page?

\newpage



```{r include=FALSE}
#page 1
```

# Introduction

The goal of this experiment is to identify the programming language that delivers the fastest execution time when calculating a value of $\pi$ with respect to Leibiniz formula. $$\sum_{n=0}^{\infty} (-1)^n/(2n+1)$$

With the increasing demand for high-performance applications, understanding which programming languages offer superior speed in terms of execution is crucial for developers, especially in domains requiring real-time processing, large-scale data analysis, and resource-intensive computations.
\
This problem will focus on evaluating a selection of popular programming languages, including but not limited to C++, C, R, Python, Java, and Ruby. The evaluation will consider how quickly a value of pi can be calculated by applying leibiniz formula up to 100000000 terms. _We are not concerned about how accurate our value of $\pi$ is, but rather how quickly a programming language computes that value_. _I think we don't need to mention this haha_ 
\

#### Compiled Language:
\
In a compiled language, the source code is translated into machine code by a compiler before execution. This machine code, often called an executable, can be run directly by the computer's hardware.
\
Compiled programs typically run faster since they are already in machine language, which the computer's processor can execute directly.
\
Examples: C, C++, Rust, and Go are examples of compiled languages.

#### Interpreted Language:
\
In an interpreted language, the source code is executed line-by-line by an interpreter at runtime. The interpreter reads the code, translates it into machine code, and executes it on the fly.
\
Interpreted programs generally run slower than compiled ones because the translation happens during execution.
\
Examples: Python, JavaScript, Ruby, and PHP are examples of interpreted languages.

#### Key Differences:
Compiled languages require a compilation step that produces an executable, while interpreted languages are executed directly by an interpreter.
\
 Compiled languages tend to have better performance due to the pre-compiled nature of the code, whereas interpreted languages are more flexible but slower due to the runtime translation.
\
Some languages, like Java, use a combination of both techniques, where the code is first compiled into an intermediate form (bytecode) and then interpreted just-in-time (JIT) at runtime.
\
_still need to edit this_


\newpage


```{r include=FALSE}
#page 2
```

# Objectives
_I don't think we need to restate our objectives here, since we've already explained it in the Introduction_

Our main objective is to test the following hypothesis: $$H_0 : \alpha_i = 0$$ $$H_1 :\text{at least of } \alpha_i \neq 0$$
$\text{For some } i = 1,2...6$

#### Planned comparsions
\
1. Which language is the fastest?
\
2. Comparsions between C and C++ and other languages
\
3. Do complied languages run faster than interpreted languages ?
\
4. Is R (vectorised Language) faster than non-vectorised language when performing calculations?
\
_what we hope to find from out experiment._
\
\newpage




```{r include=FALSE}
#page 3
```

# Sources of Variation


Our treatment Factors are 6 programming languages: C, C++, Java, Python, R and Ruby. To apply each factor, will run Leibiniz formula a billion times to get an observation. Our response $Y_{ij}$ is the amount of seconds taken to compute leibinz formula a billion times.
\
Our experimental units are PCs, which have taken from The following Labs Ishango, Scilab and MiddleTr. 

#### PC Specificiations
\
Ishango PC
\
Memory:                                     8,0 GB
\
Processor:                                  Intel® Core™ i3-9100 
\
Graphics:                                   Intel® UHD Graphics 630 (CFL GT2)
\

MidddleTR
\
Memory:                                     8,0 GB
\
Processor:                                  Intel(R) Core(TM) i5-9500 CPU
\
Graphics: 
\

ScilabB
\
Memory:                                     16,0 GB 
\
Processor:                                  12th Gen Intel(R) Core(TM) i5-12400
\
Graphics: 
\

Due to the differences in specifications in the PCs, we have decided to block for the labs, to reduce experimental error variance between the experiment units. We've selected 3 PCs from each lab (randomly).  So we have 9 experimental units in total. 
_get gpu specs for last two pcs_
 

#### Treatment Factor
_talk about treament factor_

\newpage




```{r include=FALSE}
#page 4
```

# Randomisation

_randomisation and soucres of variation can be one page_
\
_discuss process of randomisation, why we using it..._

\newpage


```{r include=FALSE}
#page 5
```

# Analysis of pilot experiment

As mentioned in the introduction, we aim to investigate the speed of various programming languages when used to perform large amount of computation. To start with, we conducted a small scale pilot experiment. This is based on the "first explore then confirm" strategy described in [1],
 
Since the number of treatments we apply in this experiment does not significantly impact the time and cost required, we apply all six treatments. However, as computers of different specifications are harder to come by, we will only use 3 different hardware setup for this pilot study.

Upon applying the Shapiro-Wilk test to the runtime of all programming languages on the same machine, we discovered that, with the exception of C and C++, the runtime of most languages tend to not follow a normal distribution.
_todo: Insesrt table for shapiro_

To address this, we run the program 5 times per sample for each programming language, and repeating the process 20 times. By the Central Limit Theorem (CLT), we assume the distribution of sample means to be normal [2]. The mean of the distribution of sample means is then an unbiased estimator for the true run time of each programming language[2], which we take as a single observation.

The process described above is automated using the following command:
```{eval=FALSE}
python3 run.py main 100000000 5 20
```

_todo: insert table for results_
\
_this is an exploratory study, so no need to list hypotheses (it's meant to generate hypotheses for the real study)_
\

_we need to decide on number of observations._
\
_full analysis here_
\
_list our hypothesis here (is there a difference or not?)_
\
_list our contrasts. Figure out whether which method we are using to control type 1 error_
\
_what we hope to find from out experiment._
\
_anova table and conclusions_
\
_contrasts and CI plus conclusions_

\newpage


```{r include=FALSE}
#page 6
```
# Our Model and Analysis

Our model:
$$Y_{ij} = \mu + \alpha_i + \beta_j+ e_{ij}$$
$$ i = 1 ...a$$
$$ j = 1 ...b$$
where
$$\sum_{i=1}^a \alpha_i = \sum_{j=1}^b \beta_i =0 $$
$$ \mu\hspace{35pt}  \text{overall mean}$$
$$\alpha_i\hspace{35pt} \text{effect of }\, i^{th}\, \text{treatment} $$
$$\beta_j\hspace{35pt} \text{effect of }\, j^{th}\, \text{block} $$
$$e_{ij}\hspace{35pt} \text{random error of the observation} $$



_need to do formatting_
  
_explain model terms_
\
_repeat steps above_


\newpage


```{r include=FALSE}
#page 7
```
# Summary and Conclusions

_draw conclusions here_
\
_explain any challenges possibly_
\
_explain how we would do it differently, to fix errors or mistakes_
\newpage

```{r include=FALSE}
#page 8
```
# References
_any references, textbook, code, people wh helped us._
\newpage

```{r include=FALSE}
#page 9
```
# Apenddix
_add important code here._
