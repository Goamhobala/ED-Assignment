---
# This template is largely functionally equivalent to that for elsevier
# The two fields that are not in the elsevier format are:
#      `corresponding_author` and `acknowledgements`
title: STA2005S - Experimental Design Assignment
date: "`r Sys.Date()`"
author:
  - name: Jing Yeh
    email: yhxjin001@myuct.ac.za
    affiliation: University of Cape Town
  - name: Saurav Sathnarayan
    email: sthsau01001@myuct.ac.za
    affiliation: University of Cape Town

abstract: |
  Test
acknowledgements: |
  This is an acknowledgement.

  It consists of two paragraphs.
keywords:
  - key
  - dictionary
  - word
bibliography: mybibfile.bib
# Use a CSL when `citation_package = "default"`
#csl: https://www.zotero.org/styles/oxford-university-press-note

## Example of some pandoc's variable to use
#fontsize: 12pt
#spacing: halfline # could also be oneline
#classoptions:
#  - endnotes
#link-citations: yes
#urlcolor: orange
#linkcolor: green
#citecolor: red
#header-includes:
#  - \usepackage[nomarkers,tablesfirst]{endfloat} # For figures and tables at end
#  - \usepackage{lineno} # For line numbering
#  - \linenumbers # For line numbering

output: 
  rticles::oup_article:
    oup_version: 0 # 1 = 2020 CTAN OUP CLS package; 0 = 2009 OUP CLS package
    extra_dependencies: booktabs # for kable example
---

```{r setup, include = FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, # By default, hide code; set to TRUE to see code
  fig.pos = 'p', # Places figures on pages separate from text
  out.width = '100%', dpi = 300, # Figure resolution and size
  fig.env = "figure" # Latex figure environment
)
library(tidyverse)
library(scales)
library(gridExtra)
library(knitr)
library(xtable)
```
\newpage

# Introduction
Computation has played a major role in human history ever since people
began living in cities. The need to calculate taxes motivated the
invention of various computing devices that aided such computations,
such as the Sumerian abacus, invented in Babylon at around
2500BC [7]. In the 21st century, the capability our digital computing
devices have vastly surpassed the capacity of those proto-computers, but
so has our need for computational power. Everything in our daily life
requires some form of computers: from our phones, cars, to even our
refrigerators (side note: initially, Java was invented for
refrigerators).

However, with large computation capability comes complexity in the
design of these devices: to speak plainly, they are damn difficult to
use. Computer scientists have therefore invented numerous
\emph{programming languages} that allow us to harness the power of these
devices more easily.

Eventually, programming languages have become the primary medium for
instructing computers to perform our increasingly complex tasks.
Understanding which programming languages offer superior execution speed
is therefore crucial for developers, especially in domains requiring
real-time processing, large-scale data analysis, and other
resource-intensive computations. The goal of this experiment is to
identify such languages that deliver the fastest execution time.

## Compiled vs Interpreted Languages
#### Compiled Language:
\
In a compiled language, the source code is translated into machine code by a compiler before execution. This machine code, often called an executable, can be run directly by the computer's hardware.
\
Compiled programs typically run faster since they are already in machine language, which the computer's processor can execute directly.
\
Examples: C, C++, Rust, and Go are examples of compiled languages.

#### Interpreted Language:
\
In an interpreted language, the source code is executed line-by-line by an interpreter at runtime. The interpreter reads the code, translates it into machine code, and executes it on the fly.
\
Interpreted programs generally run slower than compiled ones because the translation happens during execution.
\
Examples: Python, JavaScript, Ruby, and PHP are examples of interpreted languages.

#### Key Differences:
Compiled languages require a compilation step that produces an executable, while interpreted languages are executed directly by an interpreter.
\
 Compiled languages tend to have better performance due to the pre-compiled nature of the code, whereas interpreted languages are more flexible but slower due to the runtime translation.
\
Some languages, like Java, use a combination of both techniques, where the code is first compiled into an intermediate form (bytecode) and then interpreted just-in-time (JIT) at runtime.
\

## A Priori Analysis
Since existing literature on the execution times of programming languages when applying Leibniz's formula is limited, we performed an a priori test to gauge the execution time for the programming languages we planned on experimenting with. We performed 500 approximations using the algorithm for each programming language and obtained the following jittered graph.
\
\
```{r echo=FALSE, figPrior, figPrior.cap="Runtimes of Programming Languages of Interest When Applying Leibniz's Formula up to 100 million terms", fig.width=8.5}
setwd("./speed-comparison-master/data/")
middleTRNew.before <-read.csv("middleTRNew/middleTRNewBefore.csv")
middleTRNew.before.long <- pivot_longer(middleTRNew.before, cols=everything(), names_to="language", values_to="runtime")
jitter <- ggplot(middleTRNew.before.long, aes(x=language, y=log(runtime), color=language))+
  geom_point(alpha=0.3, position=position_jitter(height=1, width=0.3))+
  labs(x="language", y="runtime (log(ms))", title="Jittered Graph of Execution Time", size=8)

qq <- ggplot(middleTRNew.before.long, aes(sample=log(runtime)))+
  geom_qq(col="green")+
  geom_qq_line(col="blue")+
  facet_wrap(~language, scales="free")+
  labs(x="quantiles", y="runtime (log(ms))", title="Quantile-Quantile Plots of Execution Time",
       size=8)
grid.arrange(jitter, qq, ncol=2, nrow=1)
```
We can observe that C and C++ seem to be the fastest languages, though
further analyses are needed. We can also see from the Quantile-Quantile(Q-Q) plots that the execution times are clearly not normally distributed.

\newpage

# Reference example

Here are two sample references:. Bibliography will appear at the end of the document.

# Methods
#### Treatments:
We have 6 treatment factors, which are the programming languages we applied algorithm to. Each treatment has one level (applying the formula up to $100 \times 10^6$ terms). We selected this particular level because it is the largest, practical number of terms we could apply with our hardware setups (For some setups, it may take up to 4 hours to arrive at a single observation), and fewer terms imply larger relative measurement error [7]. We cannot include more levels because in the existing literature, most studies of such kind choose to run all languages on the same machine. However, since we would like to avoid pseudo replication as much as possible we use one machine per observation. The downside of this approach is that we do not have sufficient machines to perform more than one levels.

#### Blocks: 
From our a priori analysis, we noticed that the execution times of the 6 programming languages we tested seem to follow the same order on various hardware setups:
\begin{equation}
t(C) \approx t(C++) < t(Java) < t(R) < t(Python) < t(Ruby)
\end{equation}
Whilst the exact runtimes on machines of the same hardware setups tend to not vary much. This motivates us to block for various hardware setups. We also ensured that the machines are all operating on the same operating system, as we had later found out in the pilot experiment.

## Experimental Units: 
As mentioned earlier, we would like to avoid pseudo replication as much as possible. Therefore, we deviated from the tradition of running all programming languages on the same machine, and test only one language per machine. Our experimental units are therefore the individual machines we ran each test on.

## Sampling Procedure
Since existing literature tend to suggest that execution times of programming languages are not normally distributed, we perform a priori tests to confirm that none of our languages has normally distributed runtime. This imposed an issue as it prevented us to apply anova models. To address this, we applied the Central Limit Theorem(CLT) to obtain a normal distribution for the average execution times. We ran the program 15 times per sample for each programming language, and repeated the process 30 times. Applying CLT, it is relatively safe to assume the distribution of sample means is approximately normal [2]. If we assume sample means to be normally distributed, the mean of the distribution of sample means is then an unbiased estimator for the true run time of each programming language[2], which we will take as a single observation. (Note: We arrived at the number 15 through trial and error, and 30 from [8])

## Randomisation Procedure
We first ordered the computers belonging to each block from 1 to 6. We then used the random number generator from Python's _random_ module to randomly shuffle, and thus producing a permutation of the list, [C, C++, Java, Python, Ruby, R]. The index of each programming language in the permutation would then be paired to the computer with the same assigned number.

## Pilot Experiment
We followed this direction and performed an pilot study to obtain the following execution times of programming languages on 4 hardware setups
```{r echo=FALSE, figPilot, figPilot.cap="Interaction Graph of Programming Languages and Blocks", fig.height=2}
setwd("./speed-comparison-master/data/")

pilot <- read.csv("pilotData.csv")
pilot.long <- pivot_longer(pilot, cols=C:R, names_to = "language", values_to = "runtime")
plot <- ggplot(pilot.long, aes(x=language, y=log(runtime), group=Hardware, color=Hardware))+
  geom_point()+
  geom_line()+
  labs(title= "Interaction Graph",
       x= "Progamming Language",
       y= "Runtime (log(ms))")
plot
```
From the data collected, we observed that the results collected from Ishango do not follow the general trends established by the other three setups. Firstly, the hardware setup in Ishango lab is significantly less advance than MiddleTRNew. Yet, most programming languages tend to perform better on the Ishango machine. Secondly, to add to the first observation, not all programming languages perform better on the Ishango machine.

After further investigation, we learned that programming languages perform differently on various operating systems [4]. We hypothesised that this is likely the reason for the deviation, though further studies are needed to confirm this (we lack access to machines with the same hardware setup but run on different operating system).

Therefore, we added another constraint for selecting suitable machines: the machines must all run on Windows 10, as these machines are the most widely available.
## Design
We assume that:
$$
e_{ij} \sim \mathcal{N}(0, \sigma^2)
$$ 
We use the following anova model for our response variables:

$$
Y_{ij} = \mu + \alpha_i + \beta_j+ e_{ij}
$$
$$
i = 1 ...a
$$
$$
j = 1 ...b
$$
With the following constraints:
$$
\sum_{i=1}^a \alpha_i = \sum_{j=1}^b \beta_i =0 
$$
Where:
$$
\begin{aligned}
&\mu\hspace{35pt}  \text{overall mean} \\
&\alpha_i\hspace{35pt} \text{effect of }\, i^{th}\, \text{treatment}\\
&\beta_j\hspace{35pt} \text{effect of }\, j^{th}\, \text{block}\\
&e_{ij}\hspace{35pt} \text{random error of the observation}\\
\end{aligned}
$$

We also assume that each $e_{ij}$ is independent to each other, which allows us to assume that each $Y_{ij}$ is also independent to each other, and are normally distributed. If there are no blocking and treatment effects, then:
$$
Y_{ij} \sim \mathcal{N}(\mu, \sigma^2)
$$
Otherwise, if there are blocking and treatment effects, then:
$$
Y_{ij} \sim \mathcal{N}(\mu + \alpha_i + \beta_j, \sigma^2)
$$
Below is the layout of the design
```{r echo=FALSE, results='asis'}
df <- read.csv('diagram.csv',header = TRUE)
print(xtable(df, type = "latex", comment = FALSE, caption = 'placeholder'))
```
```{r include FALSE}
#This equation can be referenced as follows: Eq. \ref{eq:eq1}
```

\newpage
# Results
We performed the experiment described above on 7 different hardware setups and applied all 6 treatments. The table for data and details of each hardware setups can be found in the appendix. The Analysis of Variance (ANOVA) table is shown below:

```{r, tableAnova, tableAnova.cap="The ANOVA table", results='asis', warning=FALSE}
setwd("./speed-comparison-master/data/")

analysis <- read.csv("analysis.csv")
#analysis.long <- pivot_longer(analysis, cols=C:R, names_to = "language", values_to = "runtime")
analysis.long <- pivot_longer(analysis, cols=C:R, names_to = "Language", values_to = "Runtime")
anova_log <- aov(log(Runtime)~Language + Hardware, analysis.long)
frame <- data.frame(summary(anova_log)[[1]])
colnames(frame) <- c("Df", "Sum sq", "Mean sq", "F value", "Pr(>F)")
frame$'Pr(>F)' <- ifelse(is.na(frame$'Pr(>F)'), "", '< 0.0001')
frame$'F value' <- as.numeric(frame$'F value')
frame$'F value' <- ifelse(is.na(frame$'F value'), "", round(frame$'F value', digits=4) )

kable(frame, format="pandoc", escape=FALSE, digits=4, align=c("r", "r", "r", "r", "r", "r"))

anova_log.residauls <- resid(anova_log)
anova_qq <- ggplot(data.frame(anova_log.residauls), aes(sample=(anova_log.residauls)))+
  geom_qq(col="firebrick", size=2.5)+
  geom_qq_line(col="deepskyblue4", alpha=0.75)+
  labs(x="Quantile", y="Runtime log(ms)", title="Quantile-Quantile")
  

anova_residuals_fitted <- ggplot(anova_log,aes(x=fitted(anova_log), y=anova_log.residauls))+
  geom_point(col="firebrick")+
  geom_smooth(col="deepskyblue4",alpha=0 )+
  labs(x="Fitted", y="Residuals (log(ms))", title="Residuals vs Fitted")

grid.arrange(anova_qq, anova_residuals_fitted,ncol=1, nrow=2)
```
We verified our model by observing the Fitted vs Residuals graph: the residuals seem to spread out haphazardly, with no obvious patterns to be discerned. This suggests homoscedasticity [7]. Also, the Quantile-Quantile graph offers a fairly good fit.

We further verified our assumptions by performing Shapiro Wilk test on the residuals. We obtained a fairly large p-value of 0.7521, indicating that we have little evidence for residuals not being normally distributed. Further, we also got that the mean of the residuals is approximately zero ($ < 10^{-15}$), indicating that it is sensible to assume $e_{ij} \sim \mathcal{N}(0, \sigma^2)$

# References

\newpage

# Appendix
#### PC Specificiations
\
```{r echo=FALSE, results='asis'}
dfpc <- read.csv('pcSpecs.csv',header = TRUE)
print(xtable(dfpc, type = "latex", comment = FALSE, caption = 'Table of Pcs used and their respective specifications'))
```
